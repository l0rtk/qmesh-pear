# qvac-lib-infer-whispercpp

This library simplifies running inference with the Whisper transcription model within QVAC runtime applications. It provides an easy interface to load, execute, and manage Whisper inference instances, supporting multiple data sources (data loaders).

**Note: This library now uses whisper.cpp for improved performance and compatibility. The previous MLC-based implementation has been replaced.**

## Table of Contents

- [Installation](#installation)
- [Development](#development)
- [Usage](#usage)
  - [1. Choose a Data Loader](#1-choose-a-data-loader)
  - [2. Configure Transcription Parameters](#2-configure-transcription-parameters)
  - [3. Define Model Files Configuration](#3-define-model-files-configuration)
  - [4. Create Model Instance](#4-create-model-instance)
  - [5. Load Model](#5-load-model)
  - [6. Run Transcription](#6-run-transcription)
  - [7. Release Resources](#7-release-resources)
- [Usage Whisper with GSTDecoder and SileroVAD](#decoder--vad--whisper-integration-addon)
- [Quickstart example](#quickstart-example)
- [Model registry](#model-registry)
- [Other examples](#other-examples)
- [Resources](#resources)
- [License](#license)

## Installation

### Prerequisites

Make sure [Bare](#glossary) Runtime is installed:
```bash
npm install -g bare bare-make
```

Note : Make sure the Bare version is `>= 1.17.3`. Check this using : 

```bash
bare -v
```

Before proceeding with the installation, please generate a **classic GitHub Personal Access Token (PAT)** with the `read:packages` scope. Once generated, add the token to your environment variables using the name `NPM_TOKEN`.

```bash
export NPM_TOKEN=your_personal_access_token
```

Next, create a `.npmrc` file in the root of your project with the following content:

```ini
@tetherto:registry=https://npm.pkg.github.com
//npm.pkg.github.com/:_authToken=${NPM_TOKEN}
```

This configuration ensures secure access to GitHub Packages when installing scoped packages.

### Installing the Package

Install the latest development version (adjust package name based on desired model/quantization):
```bash
npm install @tetherto/transcription-whispercpp@latest
```

## Development

### Building the AddOn Locally

For local development, you'll need to build the native addon that interfaces with the Whisper model. Follow these steps:

### Prerequisites

First, make sure you have the prerequisites installed as described in the [Installation](#installation) section.

#### System Requirements

**Supported Platforms:**
- **Linux** (x64)
- **macOS** (x64, ARM64)
- **Windows** (x64)

#### Required Tools

**All Platforms:**
- **CMake** (>= 3.25)
- **Git** with submodule support
- **C++ Compiler** with C++20 support
  - Linux: GCC 9+ or Clang 10+
  - macOS: Xcode 12+ (provides Clang 12+)
  - Windows: Visual Studio 2019+ or MinGW-w64

#### vcpkg Setup

This project uses [vcpkg](https://vcpkg.io/) for C++ dependency management. You need to install and configure vcpkg before building:

**1. Install vcpkg:**

```bash
# Clone vcpkg
git clone https://github.com/Microsoft/vcpkg.git
cd vcpkg

# Bootstrap vcpkg
# On Linux/macOS:
./bootstrap-vcpkg.sh
# On Windows:
.\bootstrap-vcpkg.bat
```

**2. Set Environment Variable:**

```bash
# Linux/macOS (add to your shell profile):
export VCPKG_ROOT=/path/to/vcpkg

# Windows (PowerShell):
$env:VCPKG_ROOT = "C:\path\to\vcpkg"
# Or set permanently via System Properties > Environment Variables
```

**3. Integrate vcpkg (optional but recommended):**

```bash
# This makes vcpkg available to all CMake projects
./vcpkg integrate install
```

#### Platform-Specific Setup

**Linux:**
```bash
# Ubuntu/Debian
sudo apt update
sudo apt install build-essential cmake git pkg-config

# CentOS/RHEL/Fedora
sudo yum groupinstall "Development Tools"
sudo yum install cmake git pkgconfig
# or for newer versions:
sudo dnf groupinstall "Development Tools"
sudo dnf install cmake git pkgconfig
```

**macOS:**
```bash
# Install Xcode Command Line Tools
xcode-select --install

# Using Homebrew (recommended)
brew install cmake git

# Using MacPorts
sudo port install cmake git
```

**Windows:**
- Install [Visual Studio 2019+](https://visualstudio.microsoft.com/) with C++ development tools
- Or install [Build Tools for Visual Studio](https://visualstudio.microsoft.com/downloads/#build-tools-for-visual-studio-2019)
- Install [CMake](https://cmake.org/download/) (3.25+)
- Install [Git for Windows](https://git-scm.com/download/win)

#### GPU Acceleration (Optional)

**Metal (macOS):**
- Automatically available on macOS 10.13+ with Metal-capable hardware
- No additional setup required

**Vulkan (Linux/Windows):**
- Install Vulkan SDK from [LunarG](https://vulkan.lunarg.com/)
- Ensure your GPU drivers support Vulkan 1.1+

**Linux Vulkan Setup:**
```bash
# Ubuntu/Debian
sudo apt install vulkan-tools libvulkan-dev vulkan-validationlayers-dev spirv-tools

# CentOS/RHEL/Fedora
sudo yum install vulkan-tools vulkan-devel vulkan-validation-layers-devel spirv-tools
```

**Windows Vulkan Setup:**
- Download and install the [Vulkan SDK](https://vulkan.lunarg.com/sdk/home#windows)
- Ensure your graphics drivers are up to date

#### Clone and Setup

```bash
git clone https://github.com/tetherto/qvac-lib-infer-whispercpp.git
cd qvac-lib-infer-whispercpp

# Initialize submodules (required for native dependencies)
git submodule update --init --recursive

# Install dependencies
npm install
```

#### Build the Native AddOn

```bash
# Build the native addon
npm run build
```

This command runs the complete build sequence:
1. `bare-make generate` - Generates build configuration
2. `bare-make build` - Compiles the native C++ addon
3. `bare-make install` - Installs the built addon

#### Running Tests

After building, you can run the test suite:
```bash
# Run all tests (lint + unit + integration)
npm test

# Or run tests individually
npm run test:unit
npm run test:integration
```

#### Development Workflow

For ongoing development, the typical workflow is:
```bash
npm install && npm run build && npm run test:integration
```

## Usage

The library provides a straightforward workflow for audio transcription:

### 1. Choose a Data Loader

Data loaders abstract the way model files are accessed, whether from the filesystem, a network drive, or any other storage mechanism. More info about model registry and model builds in [resources](#resources).

- [Hyperdrive Data Loader](https://github.com/tetherto/qvac-lib-dl-hyperdrive) 
- [Filesystem Data Loader](https://github.com/tetherto/qvac-lib-dl-filesystem) 

First, select and instantiate a data loader that provides access to model files:

```javascript
// Option A: Filesystem Data Loader - for local model files
const FilesystemDL = require('@tetherto/qvac-lib-dl-filesystem')
const fsDL = new FilesystemDL({
  dirPath: './path/to/model/files' // Directory containing model weights and settings
})

// Option B: Hyperdrive Data Loader - for peer-to-peer distributed models
const HyperDriveDL = require('@tetherto/qvac-lib-dl-hyperdrive')
// Key comes from the Model Registry (see below)
const hdDL = new HyperDriveDL({
  key: 'hd://<driveKey>',  // Hyperdrive key containing model files
  store: corestore        // (Optional) A Corestore instance, If not provided, the Hyperdrive will use an in-memory store.
})
```

### 2. Configure Transcription Parameters

The transcription behavior is controlled via `whisperConfig`. Choose the `mode` based on your use case:

*   **`mode: 'batch'`**: Choose this when you want the audio processed in consistent time chunks. Good for quick, continuous transcription where perfect sentence breaks aren't critical.
As an example consider cutting a long meeting recording into regular 10-second slices (`min_seconds`/`max_seconds` control this). It gives you a steady stream of text, even if it sometimes cuts sentences awkwardly. 
*   **`mode: 'caption'`**: Choose this if you need transcriptions broken down into more natural sentences or phrases, like for video subtitles. This mode tries to detect pauses and sentence endings, making the output easier to read as captions. It's less about fixed time slices and more about creating meaningful text segments.

Set up transcription mode, output format, and timing parameters within the `whisperConfig` object:

```javascript
const args = {
  loader: hdDL, // or fsDL
  modelFileName: 'ggml-tiny.bin', // Main whisper.cpp model file
  diskPath: './models',            // Directory where model files are stored
  // Optional: Additional configuration
  opts: { 
    stats: true       // Enable performance statistics
  }
}
```

### 3. Define Model Files Configuration

The model files configuration tells the library which specific files are needed to run the translation model. With whisper.cpp, the configuration is simplified as it uses a single model file instead of separate weights and settings files.

```javascript
const config = {
  whisperConfig: {
    mode: 'batch', // 'batch' or 'caption'
                   // 'batch': Processes audio in fixed-size chunks defined by min/max_seconds.
                   // 'caption': Aims for sentence-like segmentation, less reliant on timing.
    output_format: 'plaintext', // 'plaintext', 'json', etc.
    min_seconds: 2,             // Min audio chunk duration (in seconds) for 'batch' mode.
    max_seconds: 29,            // Max audio chunk duration (in seconds) for 'batch' mode.
                               // Controls the granularity and latency vs. context trade-off.
    audio_format: 'f32le',      // Audio format: 'f32le', 's16le', 'mp3', 'wav', 'm4a', etc.
    vad_model: './path/to/ggml-silero-v5.1.2.bin' // Optional VAD model for voice activity detection
  }
}
```

**Configuration Options:**

- `modelFileName`: The main whisper.cpp model file (e.g., `ggml-tiny.bin`, `ggml-base.bin`, etc.)
- `diskPath`: Directory path where model files are stored
- `whisperConfig.audio_format`: Audio format specification for input processing
- `whisperConfig.vad_model`: Optional VAD (Voice Activity Detection) model file for improved speech segmentation

**Available Models:**

The library supports various whisper.cpp model sizes:
- `ggml-tiny.bin` - Smallest, fastest (39MB)
- `ggml-base.bin` - Balanced size/accuracy (142MB)
- `ggml-small.bin` - Better accuracy (466MB)
- `ggml-medium.bin` - High accuracy (1.5GB)
- `ggml-large.bin` - Best accuracy (3.1GB)

**VAD Model:**

For improved speech detection, you can optionally include a VAD model:
- `ggml-silero-v5.1.2.bin` - Silero VAD model for voice activity detection

Ensure that the model files are available in your chosen data loader source.

### 4. Create Model Instance

Import the specific Whisper model class based on the installed package and instantiate it:

```javascript
const TranscriptionWhispercpp = require('@tetherto/transcription-whispercpp')

const model = new TranscriptionWhispercpp(args, config)
```

Note : This import changes depending on the package installed.

### 5. Load Model

Load the model weights and initialize the inference engine. Optionally provide a callback for progress updates:

```javascript
try {
  // Basic usage
  await model.load()
  
  // Advanced usage with progress tracking
  await model.load(
    false,  // Don't close loader after loading
    (progress) => console.log(`Loading: ${progress.overallProgress}% complete`)
  )
} catch (error) {
  console.error('Failed to load model:', error)
}
```

**Progress Callback Data**

The progress callback receives an object with the following properties:

| Property | Type | Description |
|----------|------|-------------|
| `action` | string | Current operation being performed |
| `totalSize` | number | Total bytes to be loaded |
| `totalFiles` | number | Total number of files to process |
| `filesProcessed` | number | Number of files completed so far |
| `currentFile` | string | Name of file currently being processed |
| `currentFileProgress` | string | Percentage progress on current file |
| `overallProgress` | string | Overall loading progress percentage |

### 6. Run Transcription

Pass an audio stream (e.g., from `bare-fs.createReadStream`) to the `run` method. Process the transcription results asynchronously:

```javascript
try {
  // Example: Create an audio stream from a file
  const audioStream = fs.createReadStream('path/to/your/audio.ogg', {
     highWaterMark: 16000 // Adjust based on bitrate (e.g., 128000 / 8)
  })

  const response = await model.run(audioStream)

  // Option 1: Process streamed output using async iterator
  for await (const transcriptionChunk of response.iterate()) {
    console.log('Partial Transcription:', transcriptionChunk)
  }

  // Option 2: Process streamed output using callback
  await response
    .onUpdate(outputChunk => {
      console.log('Partial Transcription:', outputChunk)
    })
    .await() // Wait for transcription to complete

  console.log('Transcription finished!')

} catch (error) {
  console.error('Transcription failed:', error)
}
```

### 7. Release Resources

Always unload the model when finished to free up memory and resources:

```javascript
try {
  await model.unload()
  // If using Hyperdrive/Hyperbee, close the db instance if applicable
  await db.close()
} catch (error) {
  console.error('Failed to unload model:', error)
}
```

## Decoder + VAD + Whisper Integration AddOn

This package combines audio decoding, optional VAD trimming, and Whisper transcription into a single `TranscriptionAddon`. It automatically:

1. Decodes or ingests raw PCM/encoded audio
2. (Optionally) applies Silero VAD to drop non-speech
3. Feeds speech segments to Whisper for transcription

The principles are the same than for the single Whisper addon but with some differences in the configuration interface.

### Installation

```bash
npm install @tetherto/qvac-lib-inference-addon-whisper-tiny-q0f32-vad
```


### Configuration

When you instantiate `TranscriptionAddon`, pass:

* `loader`: your data loader instance
* `params.whisper`: exactly the same `modeParams` you use for the standalone Whisper addon
* `params.decoder.audioFormat`: one of

  * `'decoded'` (raw PCM input)
  * `'encoded'` | `'s16le'` | `'f32le'` | `'mp3'` | `'wav'` | `'m4a'`


### Usage Example

```javascript
'use strict'

const { TranscriptionAddon } = require('@tetherto/qvac-lib-inference-addon-whisper-tiny-q0f32-vad')
const HyperDriveDL               = require('@tetherto/qvac-lib-dl-hyperdrive')
const fs                         = require('bare-fs')

const audioFilePath = 'example/decodedFile.raw'
const bitRate       = 128000
const config        = require('./config.example.json')

async function main () {
  // 1. Prepare loader + model file config
  const hdDL = new HyperDriveDL({
    key: 'hd://ebfb94b378276da139554668f1ff737644eadff529c2ea0f2662d7df61fd86ca'
  })
  const hdConfig = {
    whisperConfig: {
      mode: 'caption',
      output_format: 'plaintext',
      min_seconds: 2,
      max_seconds: 6,
      audio_format: 'f32le'
    }
  }

  // 2. Instantiate the combined addon
  const transcriptionAddon = new TranscriptionAddon(
    {
      loader: hdDL,
      modelFileName: 'ggml-tiny.bin',
      diskPath: './models',
      params: {
        whisper: {
          modeParams: {
            mode:           'caption',
            updateFrequency:'on_end',
            outputFormat:   'plaintext',
            minSeconds:     2,
            maxSeconds:     6
          }
        },
        decoder: {
          audioFormat: 'decoded'  // 'encoded' (default), 'wav', 'mp3', etc.
        }
      }
    },
    hdConfig
  )

  // 3. Load everything
  await transcriptionAddon.load()

  try {
    // 4. Stream audio
    const bytesPerSecond = bitRate / 8
    const audioStream    = fs.createReadStream(audioFilePath, { highWaterMark: bytesPerSecond })

    // 5. Run and process transcription
    const response = await transcriptionAddon.run(audioStream)
    await response
      .onUpdate(output => {
        console.log('Partial Transcription Response:', output)
      })
      .await()
  } finally {
    // 6. Clean up
    await transcriptionAddon.unload()
  }

  console.log('Transcription completed.')
}

main().catch(console.error)
```

### `TranscriptionAddon` Constructor

```ts
new TranscriptionAddon(
  {
    loader: YourDataLoader,
    modelFileName: string,  // whisper.cpp model file name
    diskPath: string,       // directory path for model files
    params: {
      whisper: {
        modeParams: { /* same as standalone Whisper */ }
      },
      decoder: {
        audioFormat: 'decoded' | 'encoded' | 'wav' | 'mp3' | 's16le' | 'f32le' | 'm4a'
        sampleRate: 16000
      }
    }
  },
  {
    whisperConfig: {        // whisper configuration options
      mode: 'batch' | 'caption',
      output_format: string,
      min_seconds: number,
      max_seconds: number,
      audio_format: string,
      vad_model?: string
    }
  }
)
```

### Additional Features

- **Progress Tracking:** Monitor loading progress with callbacks
- **Performance Stats:** Measure inference time with the `stats` option

For a complete working example that brings all these steps together, see the [Quickstart Example](#quickstart-example) below.

## Quickstart example

Follow these steps to run the Quickstart demo using the Hyperdrive loader:

### 1. Clone the repo & Install the dependencies
```bash
git clone git@github.com:tetherto/qvac-lib-infer-whispercpp.git
cd qvac-lib-infer-whispercpp
npm install
```

### 2. Run the Hyperdrive example file inside `examples` folder
```bash
bare examples/example.hd.js
```
Note: It might take a few seconds for the add-on to be created and for the weights to be loaded.

### 3. Code Walkthrough

```javascript
'use strict'

const fs = require('bare-fs')
// Import the specific model class and the Hyperdrive data loader
const TranscriptionWhispercpp = require('@tetherto/transcription-whispercpp')
const HyperDriveDL = require('@tetherto/qvac-lib-dl-hyperdrive')

const audioFilePath = 'examples/decodedFile.raw'
const bitRate = 128000 // Example bitrate for audio stream chunking

async function main () {
  // Instantiate Hyperdrive Loader with the specific model key from the registry
  const hdDL = new HyperDriveDL({ key: 'hd://ebfb94b378276da139554668f1ff737644eadff529c2ea0f2662d7df61fd86ca' })

  // Define transcription parameters
  const args = {
    loader: hdDL,
    opts: {},
    modelFileName: 'ggml-tiny.bin', // Main whisper.cpp model file
    diskPath: './models',            // Directory where model files are stored
  }

  // Define model configuration
  const config = {
    whisperConfig: {
      mode: 'caption',
      output_format: 'plaintext',
      min_seconds: 2,
      max_seconds: 6,
      audio_format: 'f32le' // decodedFile.raw contains 32-bit float samples
    }
  }

  // Instantiate the Whisper model addon
  const model = new TranscriptionWhispercpp(args, config) // Pass args and config

  // Load the model weights via the data loader
  await model.load()

  try {
    // Create a readable stream for the input audio file
    const bytesPerSecond = bitRate / 8
    const audioStream = fs.createReadStream(audioFilePath, {
      highWaterMark: bytesPerSecond
    })

    // Run transcription
    const response = await model.run(audioStream)

    // Process the streaming response
    await response.onUpdate((output) => {
      console.log('Partial Transcription Response:', output)
    }).await()
  } finally {
    // Unload the model to release resources
    await model.unload()
  }
  console.log('Transcription completed.')
}

main().catch(console.error)
```

## Model registry

We use [Hyperbee](#glossary) as the model registry, mapping model identifiers (like `whisper-tiny`) to their corresponding [Hyperdrive](#glossary) keys, which point to the storage location of the model files.

*   **Hyperbee key for Whisper models registry:** `d4d762d2070f1285d012941a76f8314b243ddc99be20a4f2c72c4f2aae09070d`

The registry contains entries like:
```json
{
  "whisper-tiny":    "ebfb94b378276da139554668f1ff737644eadff529c2ea0f2662d7df61fd86ca",
}
```
Supported keys:
- whisper-tiny

## Benchmarking

We conduct comprehensive benchmarking of our Whisper transcription models to evaluate their performance across different audio conditions and metrics. The evaluations are performed using the LibriSpeech dataset, a standard benchmark for speech recognition tasks.

Our benchmarking suite measures transcription accuracy using Word Error Rate (WER) and Character Error Rate (CER), along with performance metrics such as model load times and inference speeds.

### Benchmark Results

For detailed benchmark results across all supported audio conditions and model configurations, see our [Benchmark Results Summary](benchmarks/results/results_summary.md).

The benchmarking covers:

- **Transcription Accuracy**: WER and CER scores for accuracy assessment
- **Performance Metrics**: Model loading times and inference speeds
- **Audio Conditions**: Clean speech, noisy environments and varied accents
- **Model Variants**: Different quantization levels and model sizes

Results are updated regularly as new model versions are released.

## Other examples

-   [Filesystem Data Loader](examples/example.fs.js) – Demonstrates transcription using the Filesystem loader.

## Glossary

• **Bare** – Small and modular JavaScript runtime for desktop and mobile. [Learn more](https://docs.pears.com/bare-reference/overview).  
• **QVAC** – QVAC is our open-source AI-SDK for building decentralized AI applications.  
• **Hyperdrive** – Hyperdrive is a secure, real-time distributed file system designed for easy P2P file sharing. [Learn more](https://docs.pears.com/building-blocks/hyperdrive).  
• **Hyperbee** – A decentralized B-tree built on top of Hypercores, and exposes a key-value API to store values. [Learn more](https://docs.pears.com/building-blocks/hyperbee).  
• **Corestore** – Corestore is a Hypercore factory that makes it easier to manage large collections of named Hypercores. [Learn more](https://docs.pears.com/helpers/corestore).

## Error Range
All the errors from this library are in the range of 6001-7000

## Resources

*   PoC Repo: [tetherto/qvac-transcription-poc](https://github.com/tetherto/qvac-transcription-poc)
*   Pear app (Desktop): TBD

## License

This project is licensed under the Apache-2.0 License – see the LICENSE file for details.

For questions or issues, please open an issue on the GitHub repository.